{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "except:\n",
    "    print(\"NLTK data download failed - some features may not work\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (assuming df is already loaded from data collection notebook)\n",
    "df = pd.read_csv(\"../data/combined_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6452533",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text preprocessing functions\n",
    "def advanced_text_preprocessing(text):\n",
    "    \"\"\"Comprehensive text preprocessing for NLP analysis\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    try:\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Remove words with less than 3 characters\n",
    "        tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback processing if NLTK components fail\n",
    "        words = text.split()\n",
    "        common_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "        words = [word for word in words if word not in common_stopwords and len(word) >= 3]\n",
    "        return ' '.join(words)\n",
    "\n",
    "print(\"ADVANCED TEXT PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show examples of text preprocessing\n",
    "sample_texts = df['text'].dropna().head(3).tolist()\n",
    "\n",
    "print(\"BEFORE AND AFTER PREPROCESSING EXAMPLES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(\"ORIGINAL:\")\n",
    "    print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "\n",
    "    processed = advanced_text_preprocessing(text)\n",
    "    print(\"\\nPROCESSED:\")\n",
    "    print(processed[:200] + \"...\" if len(processed) > 200 else processed)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Apply advanced preprocessing to the dataset\n",
    "print(\"\\nApplying advanced preprocessing to the entire dataset...\")\n",
    "df['title_processed'] = df['title'].apply(advanced_text_preprocessing)\n",
    "df['text_processed'] = df['text'].apply(advanced_text_preprocessing)\n",
    "\n",
    "# Calculate processing statistics\n",
    "original_title_chars = df['title'].astype(str).str.len().sum()\n",
    "processed_title_chars = df['title_processed'].str.len().sum()\n",
    "original_text_chars = df['text'].astype(str).str.len().sum()\n",
    "processed_text_chars = df['text_processed'].str.len().sum()\n",
    "\n",
    "print(f\"\\nPREPROCESSING STATISTICS:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Title characters - Original: {original_title_chars:,}, Processed: {processed_title_chars:,}\")\n",
    "print(f\"Text characters - Original: {original_text_chars:,}, Processed: {processed_text_chars:,}\")\n",
    "print(f\"Title reduction: {((original_title_chars - processed_title_chars) / original_title_chars * 100):.1f}%\")\n",
    "print(f\"Text reduction: {((original_text_chars - processed_text_chars) / original_text_chars * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e863cc0",
   "metadata": {},
   "source": [
    "## Feature Engineering for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Simple and safe feature engineering for text + title columns.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Clean text ---\n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # --- Basic text stats ---\n",
    "    df['title_length'] = df['title'].apply(len)\n",
    "    df['text_length'] = df['text'].apply(len)\n",
    "    df['title_word_count'] = df['title'].apply(lambda x: len(x.split()))\n",
    "    df['text_word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # --- Ratios ---\n",
    "    df['title_text_length_ratio'] = df['title_length'] / (df['text_length'] + 1)\n",
    "    df['title_text_word_ratio'] = df['title_word_count'] / (df['text_word_count'] + 1)\n",
    "\n",
    "    # --- Punctuation and capitalization ---\n",
    "    df['title_exclamation_count'] = df['title'].str.count('!')\n",
    "    df['title_question_count'] = df['title'].str.count(r'\\?')\n",
    "    df['text_exclamation_count'] = df['text'].str.count('!')\n",
    "    df['text_question_count'] = df['text'].str.count(r'\\?')\n",
    "    df['title_caps_ratio'] = df['title'].apply(lambda x: sum(c.isupper() for c in x) / (len(x) + 1))\n",
    "\n",
    "    # --- Special chars, digits, URLs ---\n",
    "    df['text_digit_count'] = df['text'].str.count(r'\\d')\n",
    "    df['text_special_char_count'] = df['text'].apply(lambda x: sum(not c.isalnum() and not c.isspace() for c in x))\n",
    "    df['contains_url'] = df['text'].str.contains(r'http|www|\\.com|\\.org|\\.net', case=False, na=False).astype(int)\n",
    "\n",
    "    # --- Fake news keywords ---\n",
    "    keywords = ['breaking', 'urgent', 'exclusive', 'shocking', 'leaked', 'exposed', 'viral']\n",
    "    df['fake_keywords_count'] = (\n",
    "        df['title'].apply(lambda x: sum(k in x.lower() for k in keywords)) +\n",
    "        df['text'].apply(lambda x: sum(k in x.lower() for k in keywords))\n",
    "    )\n",
    "\n",
    "    print(\"Features engineered successfully!\")\n",
    "    print(f\"Created {df.shape[1]} columns in total.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = engineer_features(df)\n",
    "\n",
    "# Show sample of new features\n",
    "print(\"\\nSample engineered features:\")\n",
    "print(df_engineered.head(5)[[\n",
    "    'title_text_length_ratio', 'title_text_word_ratio',\n",
    "    'title_exclamation_count', 'text_exclamation_count',\n",
    "    'title_caps_ratio', 'contains_url', 'fake_keywords_count'\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580801b",
   "metadata": {},
   "source": [
    "## Data Cleaning for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9262b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mojibake(text):\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # Try re-decoding bytes to fix mis-encoded text\n",
    "            text = text.encode('latin1').decode('utf-8')\n",
    "        except:\n",
    "            pass\n",
    "        # Basic cleanup\n",
    "        text = text.replace('\\xa0', ' ').replace('\\u200b', ' ')\n",
    "    return text\n",
    "\n",
    "df['text'] = (df['title'].fillna('') + ' ' + df['text'].fillna('')).apply(clean_mojibake)\n",
    "\n",
    "df = df[pd.to_numeric(df['label'], errors='coerce').notnull()]\n",
    "df['label'] = df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(series: pd.Series) -> pd.Series:\n",
    "    # Ensure everything is a string\n",
    "    series = series.fillna('').astype(str)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    series = series.str.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    series = series.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\n",
    "\n",
    "    # Remove non-word characters\n",
    "    series = series.str.replace(r'\\W', ' ', regex=True)\n",
    "\n",
    "    # Remove newlines\n",
    "    series = series.str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    series = series.str.replace(r' +', ' ', regex=True)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    series = series.str.strip()\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_text\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "df = df.dropna(subset=['title_text'])         # remove NaN texts\n",
    "df = df[df['title_text'].str.strip() != '']   # remove empty strings\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "df = df[df['label'].isin([0, 1])]\n",
    "df['label'] = df['label'].astype(int)\n",
    "print(df['title_text'].isna().sum())  # should be 0\n",
    "print(df['label'].isna().sum())       # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8689b701",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670483c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data for use in modeling\n",
    "df.to_csv('../data/preprocessed_data.csv', index=False)\n",
    "print(\"Preprocessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
