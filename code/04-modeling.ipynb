{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5162e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, roc_curve, auc, f1_score, roc_auc_score\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data (assuming df is already loaded from data collection notebook)\n",
    "df = pd.read_csv(\"../data/combined_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce387381",
   "metadata": {},
   "source": [
    "## Data Preparation and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ff20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = df[\"title_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64489389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(series: pd.Series) -> pd.Series:\n",
    "    # Ensure everything is a string\n",
    "    series = series.fillna('').astype(str)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    series = series.str.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    series = series.str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\n",
    "\n",
    "    # Remove non-word characters\n",
    "    series = series.str.replace(r'\\W', ' ', regex=True)\n",
    "\n",
    "    # Remove newlines\n",
    "    series = series.str.replace(r'\\n', '', regex=True)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    series = series.str.replace(r' +', ' ', regex=True)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    series = series.str.strip()\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "# Normalize the text data\n",
    "X_train = normalize_series(pd.Series(X_train))\n",
    "X_test = normalize_series(pd.Series(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,        # tune up to 15K for higher accuracy\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2)         # unigrams + bigrams\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "print(\"TF-IDF shape:\", X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafeb2f",
   "metadata": {},
   "source": [
    "## Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cef812",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=300, n_jobs=-1, C=3.0)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr.predict(X_test_tfidf)\n",
    "y_pred_prob_lr = lr.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "print(\"\\nLogistic Regression - Model Evaluation:\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_lr) * 100, 2), \"%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Logistic Regression model\n",
    "joblib.dump(lr, \"fake_news_lr_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "print(\"\\nModels saved successfully: 'fake_news_lr_model.pkl' and 'tfidf_vectorizer.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49996a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample\n",
    "sample = [\"Breaking: President announces new healthcare reform.\"]\n",
    "sample_tfidf = tfidf.transform(sample)\n",
    "prediction = lr.predict(sample_tfidf)[0]\n",
    "label = \"Real News\" if prediction == 1 else \"Fake News\"\n",
    "print(f\"\\nPrediction for sample:\\n→ {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88732e9",
   "metadata": {},
   "source": [
    "## Baseline Model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "y_pred_prob_xgb = xgb_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"\\nXGBoost - Model Evaluation:\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_xgb) * 100, 2), \"%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost model\n",
    "joblib.dump(xgb_model, \"fake_news_xgb_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "print(\"\\nModels saved successfully: 'fake_news_xgb_model.pkl' and 'tfidf_vectorizer.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample\n",
    "sample = [\"Breaking: President announces new healthcare reform.\"]\n",
    "sample_tfidf = tfidf.transform(sample)\n",
    "prediction = xgb_model.predict(sample_tfidf)[0]\n",
    "label = \"Real News\" if prediction == 1 else \"Fake News\"\n",
    "print(f\"\\nPrediction for sample:\\n→ {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03495f2a",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f680bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full dataset for K-fold\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words=\"english\", ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "acc_scores, f1_scores, auc_scores = [], [], []\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kfold.split(X_tfidf):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    X_train, X_test = X_tfidf[train_idx], X_tfidf[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc_score:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# Overall Performance\n",
    "print(\"\\nAverage Performance (5-Fold):\")\n",
    "print(f\"Accuracy: {np.mean(acc_scores):.4f} ± {np.std(acc_scores):.3f}\")\n",
    "print(f\"F1-score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.3f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b212f69",
   "metadata": {},
   "source": [
    "## Deep Learning Model: Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd13418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 64\n",
    "max_length = 256\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760aa994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=padding_type, truncating=trunc_type, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66163ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bi-LSTM model\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53644807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model_lstm.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=10, validation_split=0.1, verbose=1, batch_size=30, shuffle=True, callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LSTM model\n",
    "model_lstm.save(\"fake_news_bi_lstm_model.keras\")\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
    "\n",
    "# Optionally zip it to download later\n",
    "!zip -r fake_news_bi_lstm_model.zip fake_news_bi_lstm_model.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527e5f9",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history_lstm.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = history_lstm.epoch\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Loss', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, acc, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy', size=15)\n",
    "plt.xlabel('Epochs', size=15)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f9acd",
   "metadata": {},
   "source": [
    "**Training Summary**\n",
    "\n",
    "*1. Training Loss*\n",
    "* Training loss decreases steadily and consistently across epochs.\n",
    "* This indicates that the model is learning the training data well.\n",
    "\n",
    "*2. Validation Loss*\n",
    "* Validation loss decreases initially but then begins to fluctuate and slightly increase after ~3 epochs.\n",
    "* This suggests the model starts to overfit after the 3rd or 4th epoch.\n",
    "\n",
    "*3. Training Accuracy*\n",
    "* Training accuracy increases smoothly and reaches close to 0.95 by the end.\n",
    "* This is expected as the model learns patterns in the training set.\n",
    "\n",
    "*4. Validation Accuracy*\n",
    "* Validation accuracy improves initially and stabilizes around 0.88–0.90.\n",
    "* After a few epochs, it does not improve further, hinting that more training does not help generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf23a08",
   "metadata": {},
   "source": [
    "## Upload Models to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db852a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to Push Models in Hugging Face Repo for Later Evaluation\n",
    "!pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login()   # paste your HF access token here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d87d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_id = \"dl-quad/fake-news-bi-lstm-dl-quadrilateral\"\n",
    "create_repo(repo_id, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_file\n",
    "\n",
    "# Upload Logistic Regression\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/content/fake_news_lr_model.pkl\",\n",
    "    path_in_repo=\"logistic_regression/fake_news_lr_model.pkl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload Logistic Regression model\"\n",
    ")\n",
    "\n",
    "# Upload XGBoost\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/content/fake_news_xgb_model.pkl\",\n",
    "    path_in_repo=\"xgboost/fake_news_xgb_model.pkl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload XGBoost model\"\n",
    ")\n",
    "\n",
    "# Upload LSTM model\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/content/fake_news_bi_lstm_model.keras\",\n",
    "    path_in_repo=\"lstm/fake_news_bi_lstm_model.keras\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload LSTM model\"\n",
    ")\n",
    "\n",
    "# Upload tokenizer for LSTM\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/content/tokenizer.pkl\",\n",
    "    path_in_repo=\"lstm/tokenizer.pkl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload tokenizer\"\n",
    ")\n",
    "\n",
    "# Upload TF-IDF vectorizer\n",
    "upload_file(\n",
    "    path_or_fileobj=\"/content/tfidf_vectorizer.pkl\",\n",
    "    path_in_repo=\"tfidf_vectorizer/tfidf_vectorizer.pkl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload TF-IDF vectorizer\"\n",
    ")\n",
    "\n",
    "print(\"All models and artifacts uploaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
