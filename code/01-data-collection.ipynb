{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required libraries\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn nltk transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f02eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import plotly as pio\n",
    "\n",
    "# Import essential libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text processing and NLP libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0be016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge split CSV files into combined_final.csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get all split files from data directory\n",
    "data_dir = \"../data\"\n",
    "split_files = sorted(glob.glob(os.path.join(data_dir, \"split_file_*.csv\")))\n",
    "\n",
    "print(f\"Found {len(split_files)} split files:\")\n",
    "for file in split_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Read and concatenate all split files\n",
    "dfs = []\n",
    "for file in split_files:\n",
    "    df_temp = pd.read_csv(file)\n",
    "    print(f\"Loaded {os.path.basename(file)}: {df_temp.shape[0]} rows\")\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nCombined dataset: {combined_df.shape[0]} rows, {combined_df.shape[1]} columns\")\n",
    "\n",
    "# Save to combined_final.csv\n",
    "output_path = os.path.join(data_dir, \"combined_final.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved combined dataset to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ef855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined dataset\n",
    "path = \"../data/combined_final.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afe950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18137abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
